{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSGDbExff_I"
      },
      "source": [
        "# **Homework 7 - Bert (Question Answering)**\n",
        "\n",
        "If you have any questions, feel free to email us at mlta-2022-spring@googlegroups.com\n",
        "\n",
        "\n",
        "\n",
        "Slide:    [Link](https://docs.google.com/presentation/d/1H5ZONrb2LMOCixLY7D5_5-7LkIaXO6AGEaV2mRdTOMY/edit?usp=sharing)　Kaggle: [Link](https://www.kaggle.com/c/ml2022spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGOr_eS3wJJf"
      },
      "source": [
        "## Task description\n",
        "- Chinese Extractive Question Answering\n",
        "  - Input: Paragraph + Question\n",
        "  - Output: Answer\n",
        "\n",
        "- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n",
        "\n",
        "- Todo\n",
        "    - Fine tune a pretrained chinese BERT model\n",
        "    - Change hyperparameters (e.g. doc_stride)\n",
        "    - Apply linear learning rate decay\n",
        "    - Try other pretrained models\n",
        "    - Improve preprocessing\n",
        "    - Improve postprocessing\n",
        "- Training tips\n",
        "    - Automatic mixed precision\n",
        "    - Gradient accumulation\n",
        "    - Ensemble\n",
        "\n",
        "- Estimated training time (tesla t4 with automatic mixed precision enabled)\n",
        "    - Simple: 8mins\n",
        "    - Medium: 8mins\n",
        "    - Strong: 25mins\n",
        "    - Boss: 2.5hrs\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ1fSAJE2oaC"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPrc4Eie9Yo5",
        "outputId": "beeca2a1-83a8-4da2-f21b-b318613df61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb\n",
            "To: /content/hw7_data.zip\n",
            "100% 9.57M/9.57M [00:00<00:00, 242MB/s]\n",
            "Archive:  hw7_data.zip\n",
            "  inflating: hw7_dev.json            \n",
            "  inflating: hw7_test.json           \n",
            "  inflating: hw7_train.json          \n",
            "Mon May  2 09:42:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Download link 1\n",
        "!gdown --id '1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb' --output hw7_data.zip\n",
        "\n",
        "# Download Link 2 (if the above link fails) \n",
        "# !gdown --id '1qwjbRjq481lHsnTrrF4OjKQnxzgoLEFR' --output hw7_data.zip\n",
        "\n",
        "# Download Link 3 (if the above link fails) \n",
        "# !gdown --id '1QXuWjNRZH6DscSd6QcRER0cnxmpZvijn' --output hw7_data.zip\n",
        "\n",
        "!unzip -o hw7_data.zip\n",
        "\n",
        "# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TevOvhC03m0h"
      },
      "source": [
        "## Install transformers\n",
        "\n",
        "Documentation for the toolkit:　https://huggingface.co/transformers/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbxWFX_jpDom",
        "outputId": "8414b0d4-cf60-4f1c-a1bb-1b09b26cb779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.5.0 in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (0.0.49)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "# You are allowed to change version     of transformers or use other toolkits\n",
        "!pip install transformers==4.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dKM4yCh4LI_"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WOTHHtWJoahe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset \n",
        "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast, get_linear_schedule_with_warmup\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "def same_seeds(seed):\n",
        "\t  torch.manual_seed(seed)\n",
        "\t  if torch.cuda.is_available():\n",
        "\t\t    torch.cuda.manual_seed(seed)\n",
        "\t\t    torch.cuda.manual_seed_all(seed)\n",
        "\t  np.random.seed(seed)\n",
        "\t  random.seed(seed)\n",
        "\t  torch.backends.cudnn.benchmark = False\n",
        "\t  torch.backends.cudnn.deterministic = True\n",
        "same_seeds(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pBtSZP1SKQO",
        "outputId": "bc9c1865-df3f-4341-bd71-214da1abc575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.2.0 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate==0.2.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate==0.2.0) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate==0.2.0) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\t\n",
        "# GPU 在不需要這麼高精度的運算時可以轉換成 float_16（加速）\n",
        "fp16_training = True\n",
        "\n",
        "if fp16_training:\n",
        "    !pip install accelerate==0.2.0\n",
        "    from accelerate import Accelerator\n",
        "    accelerator = Accelerator(fp16=True)\n",
        "    device = accelerator.device\n",
        "\n",
        "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGnpF_hf8yAJ",
        "outputId": "f312cfe2-63d5-41d5-c6e5-2b755ba5fc05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YgXHuVLp_6j"
      },
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xyBCYGjAp3ym"
      },
      "outputs": [],
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(\"luhua/chinese_pretrain_mrc_macbert_large\").to(device)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"luhua/chinese_pretrain_mrc_macbert_large\")\n",
        "\n",
        "# hfl/chinese-macbert-base > hfl/chinese-roberta-wwm-ext(0.76) > hfl/chinese-bert-wwm(0.741)\n",
        "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Td-GTmk5OW4"
      },
      "source": [
        "## Read Data\n",
        "\n",
        "- Training set: 31690 QA pairs\n",
        "- Dev set: 4131  QA pairs\n",
        "- Test set: 4957  QA pairs\n",
        "\n",
        "- {train/dev/test}_questions:\t\n",
        "  - List of dicts with the following keys:\n",
        "   - id (int)\n",
        "   - paragraph_id (int)\n",
        "   - question_text (string)\n",
        "   - answer_text (string)\n",
        "   - answer_start (int)\n",
        "   - answer_end (int)\n",
        "- {train/dev/test}_paragraphs: \n",
        "  - List of strings\n",
        "  - paragraph_ids in questions correspond to indexs in paragraphs\n",
        "  - A paragraph may be used by several questions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NvX7hlepogvu"
      },
      "outputs": [],
      "source": [
        "def read_data(file):\n",
        "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
        "        data = json.load(reader)\n",
        "    return data[\"questions\"], data[\"paragraphs\"]\n",
        "\n",
        "train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n",
        "dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n",
        "test_questions, test_paragraphs = read_data(\"hw7_test.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm0rpTHq0e4N"
      },
      "source": [
        "## Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rTZ6B70Hoxie"
      },
      "outputs": [],
      "source": [
        "# Tokenize questions and paragraphs separately\n",
        "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
        "\n",
        "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
        "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
        "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
        "\n",
        "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False, return_offsets_mapping=True)\n",
        "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False, return_offsets_mapping=True)\n",
        "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False, return_offsets_mapping=True)\n",
        "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model\n",
        "\n",
        "dev_paragraphs = [i.replace(' ','✔').replace('\\u200b','✦').replace('\\u200e', '☺').replace('\\u3000', '☆').replace('#','●') for i in dev_paragraphs]\n",
        "test_paragraphs = [i.replace(' ','✔').replace('\\u200b','✦').replace('\\u200e', '☺').replace('\\u3000', '☆').replace('#','●') for i in test_paragraphs]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws8c8_4d5UCI"
      },
      "source": [
        "## Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Xjooag-Swnuh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class QA_Dataset(Dataset):\n",
        "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
        "        self.split = split\n",
        "        self.questions = questions\n",
        "        self.tokenized_questions = tokenized_questions\n",
        "        self.tokenized_paragraphs = tokenized_paragraphs\n",
        "        self.max_question_len = 40\n",
        "        self.max_paragraph_len = 350\n",
        "        self.counter = 0\n",
        "        \n",
        "        ##### TODO: Change value of doc_stride #####\n",
        "        # self.doc_stride = 150\n",
        "        self.doc_stride = 64\n",
        "        # 60 => 0.743\n",
        "        # 75 => 0.742\n",
        "        # 35 => 0.747\n",
        "\n",
        "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
        "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        tokenized_question = self.tokenized_questions[idx]\n",
        "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
        "        self.counter = self.counter + 1\n",
        "        ##### TODO: Preprocessing #####\n",
        "        # Hint: How to prevent model from learning something it should not learn\n",
        "        # input_id, token, attention_mask\n",
        "        # 每次 random 一個數字當作 offset\n",
        "        if self.split == \"train\":\n",
        "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
        "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
        "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
        "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
        "            offset = random.randint(0, 349-(answer_end_token - answer_start_token))\n",
        "            # 在 0（起始）與 \n",
        "            # mid 往左二分之一的最大 paragraph 長度（一般情況）、總長-最大 paragraph（最後）=> 兩者較小者\n",
        "            # 上述兩者中取最大值\n",
        "            if(self.counter % 2) == 0:\n",
        "              paragraph_start = max(0, min(answer_start_token - (offset), len(tokenized_paragraph) - self.max_paragraph_len))\n",
        "              paragraph_end = min(paragraph_start + self.max_paragraph_len, len(tokenized_paragraph))\n",
        "            else:\n",
        "              paragraph_end = min(len(tokenized_paragraph), max(answer_end_token + (offset), 0 + self.max_paragraph_len))\n",
        "              paragraph_start = max(0, paragraph_end - self.max_paragraph_len)\n",
        "            # print(paragraph_start, answer_start_token, answer_end_token, paragraph_end)\n",
        "\n",
        "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
        "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
        "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n",
        "            \n",
        "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
        "            answer_start_token += len(input_ids_question) - paragraph_start\n",
        "            answer_end_token += len(input_ids_question) - paragraph_start\n",
        "            \n",
        "            # Pad sequence and obtain inputs to model \n",
        "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
        "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
        "\n",
        "        # Validation/Testing\n",
        "        else:\n",
        "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
        "            \n",
        "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
        "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
        "                \n",
        "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
        "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
        "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
        "                \n",
        "                # Pad sequence and obtain inputs to model\n",
        "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
        "                \n",
        "                input_ids_list.append(input_ids)\n",
        "                token_type_ids_list.append(token_type_ids)\n",
        "                attention_mask_list.append(attention_mask)\n",
        "            \n",
        "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
        "\n",
        "    def padding(self, input_ids_question, input_ids_paragraph):\n",
        "        # Pad zeros if sequence length is shorter than max_seq_len\n",
        "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
        "        # Indices of input sequence tokens in the vocabulary\n",
        "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
        "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
        "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
        "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
        "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
        "        \n",
        "        return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "\n",
        "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
        "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
        "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
        "\n",
        "train_batch_size = 8\n",
        "\n",
        "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
        "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
        "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
        "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_H1kqhR8CdM"
      },
      "source": [
        "## Function for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SqeA3PLPxOHu"
      },
      "outputs": [],
      "source": [
        "def evaluate(data, output, paragraph_index, doc_stride, is_test, n_best):\n",
        "    ##### TODO: Postprocessing #####\n",
        "    # There is a bug and room for improvement in postprocessing \n",
        "    # Hint: Open your prediction file to see what is wrong \n",
        "    # check 是否存在 start index 比 end index 更後面的問題\n",
        "\n",
        "    answer = ''\n",
        "    max_prob = float('-inf')\n",
        "    num_of_windows = data[0].shape[1]\n",
        "    start_index_in_paragraph = 0\n",
        "    end_index_in_paragraph = 0\n",
        "    best_of_n_best_prob = -np.inf\n",
        "\n",
        "    for k in range(num_of_windows):\n",
        "        # Obtain answer by choosing the most probable start position / end position\n",
        "\n",
        "        mask = data[1][0][k].bool() &  data[2][0][k].bool() # token type & attention mask\n",
        "        masked_output_start = torch.masked_select(output.start_logits[k], mask.to(device))[:-1] # -1 is [SEP]\n",
        "        n_best_start_index = _get_best_indexes(masked_output_start, n_best)\n",
        "        masked_output_end = torch.masked_select(output.end_logits[k], mask.to(device))[:-1] # -1 is [SEP]\n",
        "        n_best_end_index = _get_best_indexes(masked_output_end, n_best)\n",
        "\n",
        "        masked_data = torch.masked_select(data[0][0][k], mask)[:-1]\n",
        "        for i in n_best_start_index:\n",
        "          for j in n_best_end_index:\n",
        "            if i > j :\n",
        "              continue\n",
        "            elif ((masked_output_start[i] + masked_output_end[j]) > best_of_n_best_prob) & ((j - i)<=40):\n",
        "              best_of_n_best_prob = masked_output_start[i] + masked_output_end[j]\n",
        "              start_index_in_paragraph = i + doc_stride*k\n",
        "              end_index_in_paragraph = j + doc_stride*k\n",
        "              answer = tokenizer.decode(masked_data[i : j + 1])\n",
        "\n",
        "\n",
        "    if ('[UNK]' in answer) & (is_test == False):\n",
        "      start = dev_paragraphs_tokenized['offset_mapping'][paragraph_index][start_index_in_paragraph][0]\n",
        "      end = dev_paragraphs_tokenized['offset_mapping'][paragraph_index][end_index_in_paragraph][1]\n",
        "      answer = dev_paragraphs[paragraph_index][start:end].replace('✔', ' ').replace('✦','\\u200b').replace('☺','\\u200e').replace('☆','\\u3000').replace('●','#')\n",
        "    elif ('[UNK]' in answer) & (is_test == True):\n",
        "      start = test_paragraphs_tokenized['offset_mapping'][paragraph_index][start_index_in_paragraph][0]\n",
        "      end = test_paragraphs_tokenized['offset_mapping'][paragraph_index][end_index_in_paragraph][1]\n",
        "      answer = test_paragraphs[paragraph_index][start:end].replace('✔', ' ').replace('✦','\\u200b').replace('☺','\\u200e').replace('☆','\\u3000').replace('●','#')\n",
        "\n",
        "\n",
        "    # print(f'final answer: {answer}')\n",
        "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
        "    if ('「' == answer[0]) & ('」' != answer[-1]):\n",
        "      answer = answer + '」'\n",
        "    if ('」' == answer[-1]) & ('「' != answer[0]):\n",
        "      answer = '「' + answer\n",
        "    if ('『' == answer[0]) & ('』' != answer[-1]):\n",
        "      answer = answer + '』'\n",
        "    if ('』' == answer[-1]) & ('『' != answer[0]):\n",
        "      answer = '『' + answer\n",
        "    \n",
        "    return answer.replace(' ','')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7R444eLUnN5t"
      },
      "outputs": [],
      "source": [
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzHQit6eMnKG"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f03c5ceb42a04775b2b212dabd26b581",
            "b26101b7ccfa483aadce0a9b7a4bfcde",
            "df09e38613b643e1b0b90bf5eb6a97a7",
            "6c67c08f37cc4c8599f9de908b8aa617",
            "1164678c9b0042dea1eaef7bdd16556b",
            "575ab2d1557247b0a4986f1b2f3b6cf8",
            "3c200390ff034c5bb3d6156fa4d05856",
            "cd37beca1c0a4eb6ab97cdaf178da649",
            "a5b7534a8b164b26bfda26a0742a81a4",
            "0f169b58e45349f89e07ab4068878434",
            "61a77dc112da42de9e490b7f20b98bba",
            "b3b25c271b1a40dd8eae6910f91ffbee",
            "f0eb7e1a72b847278efc55f73ac87de1",
            "5775d76fa80d4bf49f1db8efd33af719",
            "2ba5be35ec92487aab7fb1420c677252",
            "0eeaa769d3eb4ddaa604963b650b2891",
            "066cf139620149359f9519c1f3a9f549",
            "95df3f808a084bdebffdc1a249a8692f",
            "714e8dcd10f94f93a1670cbddc7804e9",
            "90c47e0a4fca49af86f529ddecc93372",
            "58edd48af8014ea5a18780d93386a1ff",
            "a6dc415a3cb34138a932f255945389d3",
            "55e4588db3f74946a35a45c6b4da5a93",
            "c8991460af454eafa04cc4c39033f01c",
            "eab6633eefe14891ac9ddc3deb62105d",
            "1c5c878042b34ee298eb0e674f30b9e5",
            "8e1296ec36e24e2c80f876f10f9f8d9d",
            "56ac525212d44c3093153e7cba686651",
            "e9a584cd632a46809f6bfdb32fe74eab",
            "40ef055783054800910225d7a3e6f218",
            "b21932cda74d464c87f76d0d653bf555",
            "8ce93dfd074e44208b3d992ed8b5fceb",
            "efb61fe4c97346f28f5f0ab3bca50df7"
          ]
        },
        "id": "3Q-B6ka7xoCM",
        "outputId": "fb612e3f-9d2e-4ac0-9851-4cd9617b3c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Training ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3962 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f03c5ceb42a04775b2b212dabd26b581"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Step 200 | loss = 0.947, acc = 0.659\n",
            "Epoch 1 | Step 400 | loss = 0.617, acc = 0.744\n",
            "Epoch 1 | Step 600 | loss = 0.579, acc = 0.758\n",
            "Epoch 1 | Step 800 | loss = 0.555, acc = 0.774\n",
            "Epoch 1 | Step 1000 | loss = 0.515, acc = 0.784\n",
            "Epoch 1 | Step 1200 | loss = 0.505, acc = 0.774\n",
            "Epoch 1 | Step 1400 | loss = 0.549, acc = 0.767\n",
            "Epoch 1 | Step 1600 | loss = 0.488, acc = 0.790\n",
            "Epoch 1 | Step 1800 | loss = 0.497, acc = 0.797\n",
            "Epoch 1 | Step 2000 | loss = 0.439, acc = 0.816\n",
            "Epoch 1 | Step 2200 | loss = 0.440, acc = 0.812\n",
            "Epoch 1 | Step 2400 | loss = 0.498, acc = 0.803\n",
            "Epoch 1 | Step 2600 | loss = 0.496, acc = 0.790\n",
            "Epoch 1 | Step 2800 | loss = 0.442, acc = 0.819\n",
            "Epoch 1 | Step 3000 | loss = 0.463, acc = 0.811\n",
            "Epoch 1 | Step 3200 | loss = 0.462, acc = 0.809\n",
            "Epoch 1 | Step 3400 | loss = 0.412, acc = 0.831\n",
            "Epoch 1 | Step 3600 | loss = 0.421, acc = 0.821\n",
            "Epoch 1 | Step 3800 | loss = 0.440, acc = 0.823\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3962 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3b25c271b1a40dd8eae6910f91ffbee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Step 200 | loss = 0.307, acc = 0.859\n",
            "Epoch 2 | Step 400 | loss = 0.230, acc = 0.894\n",
            "Epoch 2 | Step 600 | loss = 0.274, acc = 0.868\n",
            "Epoch 2 | Step 800 | loss = 0.244, acc = 0.872\n",
            "Epoch 2 | Step 1000 | loss = 0.232, acc = 0.878\n",
            "Epoch 2 | Step 1200 | loss = 0.244, acc = 0.881\n",
            "Epoch 2 | Step 1400 | loss = 0.214, acc = 0.897\n",
            "Epoch 2 | Step 1600 | loss = 0.228, acc = 0.894\n",
            "Epoch 2 | Step 1800 | loss = 0.257, acc = 0.876\n",
            "Epoch 2 | Step 2000 | loss = 0.241, acc = 0.886\n",
            "Epoch 2 | Step 2200 | loss = 0.236, acc = 0.889\n",
            "Epoch 2 | Step 2400 | loss = 0.257, acc = 0.878\n",
            "Epoch 2 | Step 2600 | loss = 0.244, acc = 0.879\n",
            "Epoch 2 | Step 2800 | loss = 0.244, acc = 0.887\n",
            "Epoch 2 | Step 3000 | loss = 0.225, acc = 0.899\n",
            "Epoch 2 | Step 3200 | loss = 0.242, acc = 0.889\n",
            "Epoch 2 | Step 3400 | loss = 0.238, acc = 0.882\n",
            "Epoch 2 | Step 3600 | loss = 0.245, acc = 0.884\n",
            "Epoch 2 | Step 3800 | loss = 0.227, acc = 0.902\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3962 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55e4588db3f74946a35a45c6b4da5a93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Step 200 | loss = 0.147, acc = 0.917\n",
            "Epoch 3 | Step 400 | loss = 0.133, acc = 0.925\n",
            "Epoch 3 | Step 600 | loss = 0.129, acc = 0.927\n",
            "Epoch 3 | Step 800 | loss = 0.127, acc = 0.931\n",
            "Epoch 3 | Step 1000 | loss = 0.150, acc = 0.924\n",
            "Epoch 3 | Step 1200 | loss = 0.148, acc = 0.928\n",
            "Epoch 3 | Step 1400 | loss = 0.153, acc = 0.921\n",
            "Epoch 3 | Step 1600 | loss = 0.126, acc = 0.933\n",
            "Epoch 3 | Step 1800 | loss = 0.157, acc = 0.926\n",
            "Epoch 3 | Step 2000 | loss = 0.143, acc = 0.924\n",
            "Epoch 3 | Step 2200 | loss = 0.120, acc = 0.944\n",
            "Epoch 3 | Step 2400 | loss = 0.138, acc = 0.925\n",
            "Epoch 3 | Step 2600 | loss = 0.133, acc = 0.933\n",
            "Epoch 3 | Step 2800 | loss = 0.141, acc = 0.929\n",
            "Epoch 3 | Step 3000 | loss = 0.136, acc = 0.936\n",
            "Epoch 3 | Step 3200 | loss = 0.147, acc = 0.916\n",
            "Epoch 3 | Step 3400 | loss = 0.136, acc = 0.927\n",
            "Epoch 3 | Step 3600 | loss = 0.148, acc = 0.922\n",
            "Epoch 3 | Step 3800 | loss = 0.146, acc = 0.925\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 3\n",
        "validation =  False\n",
        "logging_step = 200\n",
        "learning_rate = 5e-6\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "n_best = 20\n",
        "# optimizer = AdamW(model.parameters(), lr=learning_rate,\n",
        "#       weight_decay=0.01,\n",
        "#       betas=(0.9, 0.999),\n",
        "#       eps=1e-6)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 50, 11886)\n",
        "\n",
        "# accum_iter = 2\n",
        "\n",
        "if fp16_training:\n",
        "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
        "\n",
        "model.train()\n",
        "\n",
        "\n",
        "\n",
        "print(\"Start Training ...\")\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    step = 1\n",
        "    train_loss = train_acc = 0\n",
        "    \n",
        "    for batch_idx, data in enumerate(tqdm(train_loader)):\t\n",
        "        # Load all data into GPU\n",
        "        data = [i.to(device) for i in data]\n",
        "        \n",
        "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
        "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
        "        with torch.set_grad_enabled(True):\n",
        "        \n",
        "          output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
        "\n",
        "          # Choose the most probable start position / end position\n",
        "          start_index = torch.argmax(output.start_logits, dim=1)\n",
        "          end_index = torch.argmax(output.end_logits, dim=1)\n",
        "          \n",
        "          # Prediction is correct only if both start_index and end_index are correct\n",
        "          train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
        "          # output.loss = output.loss / 1\n",
        "          train_loss += output.loss\n",
        "          \n",
        "          if fp16_training:\n",
        "              accelerator.backward(output.loss)\n",
        "          else:\n",
        "              output.loss.backward()\n",
        "          \n",
        "          # if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_loader)):\n",
        "          #   optimizer.step()\n",
        "          #   optimizer.zero_grad()\n",
        "          #   scheduler.step()\n",
        "\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "          scheduler.step()\n",
        "\n",
        "          step += 1\n",
        "\n",
        "          ##### TODO: Apply linear learning rate decay #####\n",
        "\n",
        "          \n",
        "          # Print training loss and accuracy over past logging step\n",
        "          if step % logging_step == 0:\n",
        "              print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
        "              train_loss = train_acc = 0\n",
        "\n",
        "    if validation:\n",
        "            print(\"Evaluating Dev Set ...\")\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                dev_acc = 0\n",
        "                for i, data in enumerate(tqdm(dev_loader)):\n",
        "                    output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                            attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "                    # prediction is correct only if answer text exactly matches\n",
        "                    res = evaluate(data, output, dev_questions[i][\"paragraph_id\"], 64, False, n_best)\n",
        "                    if(res != dev_questions[i][\"answer_text\"]) :\n",
        "                      print(f\"wrong answering: {i}\")\n",
        "                      print(f\"guess: {res}\")\n",
        "                      print(f\"answer: {dev_questions[i]['answer_text']}\")\n",
        "                      print()\n",
        "                    dev_acc += res == dev_questions[i][\"answer_text\"]\n",
        "                print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
        "            model.train()\n",
        "    \n",
        "\n",
        "# Save a model and its configuration file to the directory 「saved_model」 \n",
        "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
        "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12BKkGhPwQW4",
        "outputId": "88c9c8d3-9c9b-41c2-ef4e-53ebbadd89b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model ...\n"
          ]
        }
      ],
      "source": [
        "print(\"Saving Model ...\")\n",
        "model_save_dir = \"./saved_model/luhua_macbert_third\" \n",
        "model.save_pretrained(model_save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "19060601cfd047f3be6a9af302db91f7",
            "803b8f302e184a46862b2aa520b1ded7",
            "22998c26cb7d4cef9abce2bb42afe4f4",
            "54393a292ac643a0a2091e9d1cd1936c",
            "653d9a825d8245a39a7bf7a4e1d09d8d",
            "1e8a8ffe09a54d8dbdb1579e1631b4bb",
            "d8353b2f87c04123bb7976905d92dbe9",
            "1918b7bfe2df46c9b5a0590707e23c92",
            "524507bf667244329ff1eb14bdc4f132",
            "d4bc5d89484f40b2ba245981a322875c",
            "0a8551ead130452080293406ea8ec2da"
          ]
        },
        "id": "ajibODQCegkA",
        "outputId": "b9d2aa11-4d78-4618-f0af-123f9d199e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Test Set ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19060601cfd047f3be6a9af302db91f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed! Result is in result.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating Test Set ...\")\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(\"luhua/chinese_pretrain_mrc_macbert_large\")\n",
        "result = []\n",
        "n_best = 20\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(tqdm(test_loader)):\n",
        "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        result.append(evaluate(data, output, test_questions[i][\"paragraph_id\"], 64, is_test = True, n_best = n_best))\n",
        "\n",
        "result_file = \"result.csv\"\n",
        "with open(result_file, 'w') as f:\t\n",
        "\t  f.write(\"ID,Answer\\n\")\n",
        "\t  for i, test_question in enumerate(test_questions):\n",
        "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
        "        # Answers in kaggle are processed in the same way\n",
        "\t\t    f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
        "\n",
        "print(f\"Completed! Result is in {result_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nhvw3raz1BIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "6040c350-32c7-4306-a230-ac97e88f6bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "404 Client Error: Not Found for url: https://huggingface.co/./saved_model/luhua_macbert_82775/resolve/main/config.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/./saved_model/luhua_macbert_82775/resolve/main/config.json",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5cc1ce4efa78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./saved_model/luhua_macbert_82775\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./saved_model/luhua_macbert_83017\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./saved_model/luhua_macbert_second\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m                 \u001b[0m_from_auto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_auto_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m             )\n\u001b[1;32m    990\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             assert (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             )\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load config for './saved_model/luhua_macbert_82775'. Make sure that:\n\n- './saved_model/luhua_macbert_82775' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or './saved_model/luhua_macbert_82775' is the correct path to a directory containing a config.json file\n\n"
          ]
        }
      ],
      "source": [
        "model_2 = BertForQuestionAnswering.from_pretrained(\"./saved_model/luhua_macbert_82775\").to(device)\n",
        "model_3 = BertForQuestionAnswering.from_pretrained(\"./saved_model/luhua_macbert_83017\").to(device)\n",
        "model_4 = BertForQuestionAnswering.from_pretrained(\"./saved_model/luhua_macbert_second\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCmXp4cI1X-h"
      },
      "outputs": [],
      "source": [
        "def evaluate_ensemble(data, output, output2, output3, output4, paragraph_index, doc_stride, is_test, n_best):\n",
        "    ##### TODO: Postprocessing #####\n",
        "    # There is a bug and room for improvement in postprocessing \n",
        "    # Hint: Open your prediction file to see what is wrong \n",
        "    # check 是否存在 start index 比 end index 更後面的問題\n",
        "\n",
        "    # \n",
        "\n",
        "    answer = ''\n",
        "    max_prob = float('-inf')\n",
        "    num_of_windows = data[0].shape[1]\n",
        "    start_index_in_paragraph = 0\n",
        "    end_index_in_paragraph = 0\n",
        "    best_of_n_best_prob = -np.inf\n",
        "    start_logits = (output.start_logits + output2.start_logits + output3.start_logits + output4.start_logits)/4\n",
        "    end_logits = (output.end_logits + output2.end_logits + output3.end_logits + output4.end_logits)/4\n",
        "\n",
        "    for k in range(num_of_windows):\n",
        "        # Obtain answer by choosing the most probable start position / end position\n",
        "        mask = data[1][0][k].bool() &  data[2][0][k].bool() # token type & attention mask\n",
        "        masked_output_start = torch.masked_select(start_logits[k], mask.to(device))[:-1] # -1 is [SEP]\n",
        "        n_best_start_index = _get_best_indexes(masked_output_start, n_best)\n",
        "        masked_output_end = torch.masked_select(end_logits[k], mask.to(device))[:-1] # -1 is [SEP]\n",
        "        n_best_end_index = _get_best_indexes(masked_output_end, n_best)\n",
        "\n",
        "        # prob = start_prob + end_prob\n",
        "        masked_data = torch.masked_select(data[0][0][k], mask)[:-1]\n",
        "        for i in n_best_start_index:\n",
        "          for j in n_best_end_index:\n",
        "            if i > j :\n",
        "              continue\n",
        "            elif ((masked_output_start[i] + masked_output_end[j]) > best_of_n_best_prob) & ((j - i)<=40):\n",
        "              best_of_n_best_prob = masked_output_start[i] + masked_output_end[j]\n",
        "              start_index_in_paragraph = i + doc_stride*k\n",
        "              end_index_in_paragraph = j + doc_stride*k\n",
        "              answer = tokenizer.decode(masked_data[i : j + 1])\n",
        "\n",
        "\n",
        "    if ('[UNK]' in answer) & (is_test == False):\n",
        "      start = dev_paragraphs_tokenized['offset_mapping'][paragraph_index][start_index_in_paragraph][0]\n",
        "      end = dev_paragraphs_tokenized['offset_mapping'][paragraph_index][end_index_in_paragraph][1]\n",
        "      answer = dev_paragraphs[paragraph_index][start:end].replace('✔', ' ').replace('✦','\\u200b').replace('☺','\\u200e').replace('☆','\\u3000').replace('●','#')\n",
        "    elif ('[UNK]' in answer) & (is_test == True):\n",
        "      start = test_paragraphs_tokenized['offset_mapping'][paragraph_index][start_index_in_paragraph][0]\n",
        "      end = test_paragraphs_tokenized['offset_mapping'][paragraph_index][end_index_in_paragraph][1]\n",
        "      answer = test_paragraphs[paragraph_index][start:end].replace('✔', ' ').replace('✦','\\u200b').replace('☺','\\u200e').replace('☆','\\u3000').replace('●','#')\n",
        "\n",
        "    if ('「' == answer[0]) & ('」' != answer[-1]):\n",
        "      answer = answer + '」'\n",
        "    if ('」' == answer[-1]) & ('「' != answer[0]):\n",
        "      answer = '「' + answer\n",
        "    if ('『' == answer[0]) & ('』' != answer[-1]):\n",
        "      answer = answer + '』'\n",
        "    if ('』' == answer[-1]) & ('『' != answer[0]):\n",
        "      answer = '『' + answer\n",
        "    \n",
        "    return answer.replace(' ','')\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btYFiO3E12jl"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating Test Set ...\")\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(\"luhua/chinese_pretrain_mrc_macbert_large\")\n",
        "result = []\n",
        "n_best = 20\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(tqdm(test_loader)):\n",
        "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        output_2 = model_2(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        output_3 = model_3(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        output_4 = model_3(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        result.append(evaluate_ensemble(data, output, output_2, output_3, output_4, test_questions[i][\"paragraph_id\"], 64, is_test = True, n_best = n_best))\n",
        "\n",
        "result_file = \"result.csv\"\n",
        "with open(result_file, 'w') as f:\t\n",
        "\t  f.write(\"ID,Answer\\n\")\n",
        "\t  for i, test_question in enumerate(test_questions):\n",
        "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
        "        # Answers in kaggle are processed in the same way\n",
        "\t\t    f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
        "\n",
        "print(f\"Completed! Result is in {result_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMmdLOKBMsdE"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P79MZR_0MJc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('result.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/sample_data')\n",
        "!git init\n",
        "!git config --global user.email \"mkop9456@gmail.com\"\n",
        "!git config --global user.name \"Ohlamesaint\"\n",
        "!git config --global user.password \"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\"\n",
        "!git remote set-url origin https://Ohlamesaint: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@github.com/Ohlamesaint/ML2022.git\n",
        "!git remote -v\n",
        "!git fetch origin\n",
        "!git merge origin/main --allow-unrelated-histories\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uwgxI-AcojsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add README.md\n",
        "!git commit -a -m 'test'\n",
        "!git push origin HEAD:main"
      ],
      "metadata": {
        "id": "suE3TTxDrO2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ML2022Spring_HW7_82492.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f03c5ceb42a04775b2b212dabd26b581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b26101b7ccfa483aadce0a9b7a4bfcde",
              "IPY_MODEL_df09e38613b643e1b0b90bf5eb6a97a7",
              "IPY_MODEL_6c67c08f37cc4c8599f9de908b8aa617"
            ],
            "layout": "IPY_MODEL_1164678c9b0042dea1eaef7bdd16556b"
          }
        },
        "b26101b7ccfa483aadce0a9b7a4bfcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_575ab2d1557247b0a4986f1b2f3b6cf8",
            "placeholder": "​",
            "style": "IPY_MODEL_3c200390ff034c5bb3d6156fa4d05856",
            "value": "100%"
          }
        },
        "df09e38613b643e1b0b90bf5eb6a97a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd37beca1c0a4eb6ab97cdaf178da649",
            "max": 3962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5b7534a8b164b26bfda26a0742a81a4",
            "value": 3962
          }
        },
        "6c67c08f37cc4c8599f9de908b8aa617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f169b58e45349f89e07ab4068878434",
            "placeholder": "​",
            "style": "IPY_MODEL_61a77dc112da42de9e490b7f20b98bba",
            "value": " 3962/3962 [1:12:56&lt;00:00,  1.14it/s]"
          }
        },
        "1164678c9b0042dea1eaef7bdd16556b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "575ab2d1557247b0a4986f1b2f3b6cf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c200390ff034c5bb3d6156fa4d05856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd37beca1c0a4eb6ab97cdaf178da649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5b7534a8b164b26bfda26a0742a81a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f169b58e45349f89e07ab4068878434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a77dc112da42de9e490b7f20b98bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3b25c271b1a40dd8eae6910f91ffbee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0eb7e1a72b847278efc55f73ac87de1",
              "IPY_MODEL_5775d76fa80d4bf49f1db8efd33af719",
              "IPY_MODEL_2ba5be35ec92487aab7fb1420c677252"
            ],
            "layout": "IPY_MODEL_0eeaa769d3eb4ddaa604963b650b2891"
          }
        },
        "f0eb7e1a72b847278efc55f73ac87de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_066cf139620149359f9519c1f3a9f549",
            "placeholder": "​",
            "style": "IPY_MODEL_95df3f808a084bdebffdc1a249a8692f",
            "value": "100%"
          }
        },
        "5775d76fa80d4bf49f1db8efd33af719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_714e8dcd10f94f93a1670cbddc7804e9",
            "max": 3962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90c47e0a4fca49af86f529ddecc93372",
            "value": 3962
          }
        },
        "2ba5be35ec92487aab7fb1420c677252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58edd48af8014ea5a18780d93386a1ff",
            "placeholder": "​",
            "style": "IPY_MODEL_a6dc415a3cb34138a932f255945389d3",
            "value": " 3962/3962 [1:12:55&lt;00:00,  1.14it/s]"
          }
        },
        "0eeaa769d3eb4ddaa604963b650b2891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066cf139620149359f9519c1f3a9f549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95df3f808a084bdebffdc1a249a8692f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "714e8dcd10f94f93a1670cbddc7804e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c47e0a4fca49af86f529ddecc93372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58edd48af8014ea5a18780d93386a1ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6dc415a3cb34138a932f255945389d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55e4588db3f74946a35a45c6b4da5a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8991460af454eafa04cc4c39033f01c",
              "IPY_MODEL_eab6633eefe14891ac9ddc3deb62105d",
              "IPY_MODEL_1c5c878042b34ee298eb0e674f30b9e5"
            ],
            "layout": "IPY_MODEL_8e1296ec36e24e2c80f876f10f9f8d9d"
          }
        },
        "c8991460af454eafa04cc4c39033f01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56ac525212d44c3093153e7cba686651",
            "placeholder": "​",
            "style": "IPY_MODEL_e9a584cd632a46809f6bfdb32fe74eab",
            "value": "100%"
          }
        },
        "eab6633eefe14891ac9ddc3deb62105d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40ef055783054800910225d7a3e6f218",
            "max": 3962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b21932cda74d464c87f76d0d653bf555",
            "value": 3962
          }
        },
        "1c5c878042b34ee298eb0e674f30b9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ce93dfd074e44208b3d992ed8b5fceb",
            "placeholder": "​",
            "style": "IPY_MODEL_efb61fe4c97346f28f5f0ab3bca50df7",
            "value": " 3962/3962 [1:12:55&lt;00:00,  1.14it/s]"
          }
        },
        "8e1296ec36e24e2c80f876f10f9f8d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ac525212d44c3093153e7cba686651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9a584cd632a46809f6bfdb32fe74eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40ef055783054800910225d7a3e6f218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b21932cda74d464c87f76d0d653bf555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ce93dfd074e44208b3d992ed8b5fceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efb61fe4c97346f28f5f0ab3bca50df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19060601cfd047f3be6a9af302db91f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_803b8f302e184a46862b2aa520b1ded7",
              "IPY_MODEL_22998c26cb7d4cef9abce2bb42afe4f4",
              "IPY_MODEL_54393a292ac643a0a2091e9d1cd1936c"
            ],
            "layout": "IPY_MODEL_653d9a825d8245a39a7bf7a4e1d09d8d"
          }
        },
        "803b8f302e184a46862b2aa520b1ded7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e8a8ffe09a54d8dbdb1579e1631b4bb",
            "placeholder": "​",
            "style": "IPY_MODEL_d8353b2f87c04123bb7976905d92dbe9",
            "value": "100%"
          }
        },
        "22998c26cb7d4cef9abce2bb42afe4f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1918b7bfe2df46c9b5a0590707e23c92",
            "max": 4957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_524507bf667244329ff1eb14bdc4f132",
            "value": 4957
          }
        },
        "54393a292ac643a0a2091e9d1cd1936c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4bc5d89484f40b2ba245981a322875c",
            "placeholder": "​",
            "style": "IPY_MODEL_0a8551ead130452080293406ea8ec2da",
            "value": " 4957/4957 [1:27:45&lt;00:00,  1.16it/s]"
          }
        },
        "653d9a825d8245a39a7bf7a4e1d09d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e8a8ffe09a54d8dbdb1579e1631b4bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8353b2f87c04123bb7976905d92dbe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1918b7bfe2df46c9b5a0590707e23c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "524507bf667244329ff1eb14bdc4f132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4bc5d89484f40b2ba245981a322875c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a8551ead130452080293406ea8ec2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}